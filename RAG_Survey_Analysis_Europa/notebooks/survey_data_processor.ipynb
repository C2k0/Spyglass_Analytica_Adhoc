{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Survey Data Processor with Simple Dictionary Mappings\n\nThis notebook processes survey data from SQL query files and applies dictionary-based transformations.\n\n## Overview\n- Reads SQL query files for each survey\n- Applies dictionary mappings to transform text responses to numeric values\n- Uses simple JSON configuration files to specify which columns need which mappings\n- Outputs combined files with transformed data for analysis",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport sqlite3\nimport os\nimport json\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Add parent directory to path for imports\nimport sys\nsys.path.append('../')\n\nfrom shared.metrics.common_metrics import *\nfrom shared.utils.data_validator import *\nfrom shared.transformations import SimpleSurveyMapper, quick_transform"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\n### Survey Configuration\n- Each survey has a folder in `sql_queries/survey_name/` containing:\n  - `get_data.sql` - SQL query to extract survey data\n  - `dev_notes.md` - Developer documentation for the SQL query\n- Column mappings are defined in `config/survey_column_mappings.json`\n- Dictionary mappings are defined in `config/mapping_dictionaries.json`",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Survey Configuration - Update this list with your survey names\nSURVEYS = [\n    \"survey_1\",\n    \"survey_2\", \n    \"survey_3\",\n    \"survey_4\",\n    \"survey_5\"\n]\n\n# Initialize the mapper\nmapper = SimpleSurveyMapper()\n\n# File paths\nSQL_QUERIES_DIR = \"../sql_queries/\"\nOUTPUT_DIR = \"../data/processed/\"\nTIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# Ensure output directory exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Processing {len(SURVEYS)} surveys\")\nprint(f\"\\nAvailable mapping dictionaries ({len(mapper.get_available_mappings())}):\")\nfor i, mapping_name in enumerate(mapper.get_available_mappings(), 1):\n    print(f\"  {i}. {mapping_name}\")\n\n# Show example mappings\nprint(\"\\nExample mappings:\")\nprint(\"  satisfaction_5_scale: 'very satisfied' → 5, 'satisfied' → 4, ...\")\nprint(\"  yes_no_binary: 'yes' → 1, 'no' → 0\")\nprint(\"  recommendation_10_scale: '0' → 0, '1' → 1, ..., '10' → 10\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Loading Functions",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def validate_columns(df, survey_name, all_columns_dict):\n    \"\"\"\n    Validate that all columns in the DataFrame are in the allowed columns dictionary.\n    \n    Args:\n        df (pd.DataFrame): Survey data\n        survey_name (str): Name of the survey\n        all_columns_dict (dict): Dictionary of allowed column names and their types\n    \n    Returns:\n        tuple: (is_valid, invalid_columns)\n    \"\"\"\n    # Exclude automatically added columns from validation\n    auto_added_columns = {'Survey_Name', 'Processed_Date'}\n    df_columns = set(df.columns) - auto_added_columns\n    allowed_columns = set(all_columns_dict.keys())\n    \n    invalid_columns = df_columns - allowed_columns\n    \n    if invalid_columns:\n        print(f\"  ✗ VALIDATION FAILED for {survey_name}\")\n        print(f\"    Invalid columns: {sorted(invalid_columns)}\")\n        print(f\"    Valid columns must be from: {sorted(allowed_columns)}\")\n        return False, invalid_columns\n    else:\n        print(f\"  ✓ Column validation passed for {survey_name}\")\n        return True, set()\n\ndef load_sql_query(survey_name):\n    \"\"\"\n    Load SQL query from file for a given survey.\n    \n    Args:\n        survey_name (str): Name of the survey\n    \n    Returns:\n        str: SQL query string\n    \"\"\"\n    sql_file_path = os.path.join(SQL_QUERIES_DIR, survey_name, \"get_data.sql\")\n    \n    if not os.path.exists(sql_file_path):\n        raise FileNotFoundError(f\"SQL file not found: {sql_file_path}\")\n    \n    with open(sql_file_path, 'r', encoding='utf-8') as file:\n        query = file.read().strip()\n    \n    return query\n\ndef execute_query_and_load_data(survey_name, connection=None):\n    \"\"\"\n    Execute SQL query and load data for a survey.\n    \n    Args:\n        survey_name (str): Name of the survey\n        connection: Database connection (if None, will create SQLite connection)\n    \n    Returns:\n        pd.DataFrame: Survey data\n    \"\"\"\n    query = load_sql_query(survey_name)\n    \n    # If no connection provided, create a simple SQLite connection\n    # Note: Update this section based on your actual database configuration\n    if connection is None:\n        # For demonstration - replace with your actual database connection\n        conn = sqlite3.connect(':memory:')\n        print(f\"Warning: Using in-memory SQLite for {survey_name}. Update connection for production.\")\n    else:\n        conn = connection\n    \n    try:\n        df = pd.read_sql_query(query, conn)\n        \n        # Validate columns before processing\n        is_valid, invalid_cols = validate_columns(df, survey_name, ALL_COLUMNS)\n        if not is_valid:\n            print(f\"  ✗ Skipping {survey_name} due to column validation failure\")\n            return None\n        \n        df['Survey_Name'] = survey_name  # Add survey identifier\n        print(f\"✓ Loaded {len(df)} records from {survey_name}\")\n        return df\n    except Exception as e:\n        print(f\"✗ Error loading data for {survey_name}: {str(e)}\")\n        return None\n    finally:\n        if connection is None and 'conn' in locals():\n            conn.close()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "## View Survey Mappings\n\nLet's examine what transformations are configured for each survey.\n\n# Display configured mappings for each survey\nfor survey_name in SURVEYS[:3]:  # Show first 3 surveys\n    print(f\"\\n{survey_name}:\")\n    print(\"-\" * 50)\n    \n    mappings = mapper.get_survey_mappings(survey_name)\n    if mappings:\n        for column, dict_name in mappings.items():\n            print(f\"  {column} → {dict_name}\")\n    else:\n        print(\"  No mappings configured\")\n\nprint(\"\\n... and more surveys\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage for processed data\n",
    "all_survey_data = []\n",
    "nps_driver_data = []\n",
    "processing_summary = []\n",
    "\n",
    "print(\"Starting data processing pipeline...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Process each survey\nfor survey_name in SURVEYS:\n    print(f\"\\nProcessing survey: {survey_name}\")\n    print(\"=\" * 60)\n    \n    try:\n        # Load data from SQL query\n        df = execute_query_and_load_data(survey_name)\n        \n        if df is not None and len(df) > 0:\n            # Store original data structure info\n            original_shape = df.shape\n            print(f\"  Original shape: {original_shape}\")\n            \n            # Add survey identifier and timestamp\n            df['Survey_Name'] = survey_name\n            df['Processed_Date'] = datetime.now()\n            \n            # Show configured mappings for this survey\n            survey_mappings = mapper.get_survey_mappings(survey_name)\n            if survey_mappings:\n                print(f\"  Configured mappings: {len(survey_mappings)} columns\")\n                \n                # Apply transformations using the simple mapper\n                df = mapper.transform_survey(df, survey_name, verbose=False)\n                print(f\"  ✓ Transformations applied\")\n            else:\n                print(f\"  ⚠ No mappings configured for this survey\")\n            \n            # Validate that expected columns were transformed\n            validation = mapper.validate_survey_config(survey_name, df)\n            if validation['missing']:\n                print(f\"  ⚠ Missing columns: {validation['missing']}\")\n            \n            # Store full dataset\n            all_survey_data.append(df.copy())\n            \n            # Extract LTR and driver scores if available\n            ltr_driver_subset = extract_ltr_and_drivers(df, survey_name, ALL_COLUMNS)\n            if ltr_driver_subset is not None:\n                nps_driver_data.append(ltr_driver_subset)\n            \n            # Record processing summary\n            processing_summary.append({\n                'Survey_Name': survey_name,\n                'Records_Processed': len(df),\n                'Columns_Count': len(df.columns),\n                'Columns_Transformed': len(survey_mappings),\n                'Has_LTR_Data': any(ALL_COLUMNS.get(col) == \"LTR\" for col in df.columns),\n                'Has_Driver_Data': any(ALL_COLUMNS.get(col) == \"DRIVERS\" for col in df.columns),\n                'Processing_Status': 'Success',\n                'Processed_Time': datetime.now()\n            })\n            \n            print(f\"  ✓ Successfully processed {len(df)} records\")\n            \n        else:\n            status = 'No Data' if df is not None else 'Failed to load'\n            print(f\"  ✗ {status} for {survey_name}\")\n            processing_summary.append({\n                'Survey_Name': survey_name,\n                'Records_Processed': 0,\n                'Columns_Count': 0,\n                'Columns_Transformed': 0,\n                'Has_LTR_Data': False,\n                'Has_Driver_Data': False,\n                'Processing_Status': status,\n                'Processed_Time': datetime.now()\n            })\n            \n    except Exception as e:\n        print(f\"  ✗ Error processing {survey_name}: {str(e)}\")\n        processing_summary.append({\n            'Survey_Name': survey_name,\n            'Records_Processed': 0,\n            'Columns_Count': 0,\n            'Columns_Transformed': 0,\n            'Has_LTR_Data': False,\n            'Has_Driver_Data': False,\n            'Processing_Status': f'Error: {str(e)}',\n            'Processed_Time': datetime.now()\n        })\n\nprint(f\"\\n\\nData processing complete. Processed {len(all_survey_data)} surveys successfully.\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def extract_ltr_and_drivers(df, survey_name, all_columns_dict):\n    \"\"\"\n    Extract LTR (Likelihood to Recommend) and driver scores from survey data.\n    \n    Args:\n        df (pd.DataFrame): Survey data\n        survey_name (str): Name of the survey\n        all_columns_dict (dict): Dictionary mapping column names to types\n    \n    Returns:\n        pd.DataFrame or None: Subset with LTR and driver data\n    \"\"\"\n    # Get LTR and DRIVERS columns from the column type mapping\n    ltr_columns = [col for col, col_type in all_columns_dict.items() \n                   if col_type == \"LTR\" and col in df.columns]\n    driver_columns = [col for col, col_type in all_columns_dict.items() \n                      if col_type == \"DRIVERS\" and col in df.columns]\n    \n    # Standard columns to include\n    standard_columns = [col for col, col_type in all_columns_dict.items() \n                       if col_type == \"STANDARD\" and col in df.columns]\n    \n    # Auto-added columns to include\n    auto_added_columns = ['Survey_Name', 'Processed_Date']\n    available_auto_added = [col for col in auto_added_columns if col in df.columns]\n    \n    # Combine all relevant columns\n    relevant_columns = list(set(standard_columns + ltr_columns + driver_columns + available_auto_added))\n    \n    if len(ltr_columns) > 0 or len(driver_columns) > 0:\n        subset_df = df[relevant_columns].copy()\n        print(f\"    Extracted LTR/Driver data: {len(ltr_columns)} LTR columns, {len(driver_columns)} driver columns\")\n        print(f\"    LTR columns: {ltr_columns}\")\n        print(f\"    Driver columns: {driver_columns}\")\n        return subset_df\n    else:\n        print(f\"    No LTR or driver columns found in {survey_name}\")\n        return None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Generating output files...\\n\")\n\n# Define output paths\nRAG_OUTPUT_DIR = \"../rag_outputs/current/\"\nARCHIVE_DIR = \"../rag_outputs/archive/\"\nos.makedirs(RAG_OUTPUT_DIR, exist_ok=True)\nos.makedirs(ARCHIVE_DIR, exist_ok=True)\n\n# Output 1: Combined LTR and Driver Scores\nif nps_driver_data:\n    combined_ltr_drivers = pd.concat(nps_driver_data, ignore_index=True, sort=False)\n    \n    # Save to archive with timestamp\n    archive_file = os.path.join(ARCHIVE_DIR, f\"Combined_LTR_Drivers_{TIMESTAMP}.csv\")\n    combined_ltr_drivers.to_csv(archive_file, index=False)\n    \n    # Save to current for RAG access\n    current_file = os.path.join(RAG_OUTPUT_DIR, \"combined_ltr_drivers.csv\")\n    combined_ltr_drivers.to_csv(current_file, index=False)\n    \n    print(f\"✓ LTR & Driver Scores:\")\n    print(f\"  - RAG access: {current_file}\")\n    print(f\"  - Archive: {archive_file}\")\n    print(f\"  - Total records: {len(combined_ltr_drivers)}\")\n    print(f\"  - Surveys included: {combined_ltr_drivers['Survey_Name'].nunique()}\")\n    print(f\"  - Columns: {len(combined_ltr_drivers.columns)}\")\nelse:\n    print(\"✗ No LTR/Driver data found across surveys\")\n\nprint()\n\n# Output 2: All Survey Data Combined\nif all_survey_data:\n    combined_all_data = pd.concat(all_survey_data, ignore_index=True, sort=False)\n    \n    # Save to archive with timestamp\n    archive_file = os.path.join(ARCHIVE_DIR, f\"Combined_All_Surveys_{TIMESTAMP}.csv\")\n    combined_all_data.to_csv(archive_file, index=False)\n    \n    # Save to current for RAG access\n    current_file = os.path.join(RAG_OUTPUT_DIR, \"combined_all_surveys.csv\")\n    combined_all_data.to_csv(current_file, index=False)\n    \n    print(f\"✓ All Survey Data:\")\n    print(f\"  - RAG access: {current_file}\")\n    print(f\"  - Archive: {archive_file}\")\n    print(f\"  - Total records: {len(combined_all_data)}\")\n    print(f\"  - Surveys included: {combined_all_data['Survey_Name'].nunique()}\")\n    print(f\"  - Total columns: {len(combined_all_data.columns)}\")\nelse:\n    print(\"✗ No survey data processed successfully\")\n\nprint()\n\n# Output 3: Processing Summary\nsummary_df = pd.DataFrame(processing_summary)\n\n# Save to archive with timestamp\narchive_file = os.path.join(ARCHIVE_DIR, f\"Processing_Summary_{TIMESTAMP}.csv\")\nsummary_df.to_csv(archive_file, index=False)\n\n# Save to current for RAG access\ncurrent_file = os.path.join(RAG_OUTPUT_DIR, \"processing_summary.csv\")\nsummary_df.to_csv(current_file, index=False)\n\nprint(f\"✓ Processing Summary:\")\nprint(f\"  - RAG access: {current_file}\")\nprint(f\"  - Archive: {archive_file}\")\nprint(f\"  - Surveys processed: {len(summary_df)}\")\nprint(f\"  - Successful: {len(summary_df[summary_df['Processing_Status'] == 'Success'])}\")\nprint(f\"  - Failed: {len(summary_df[summary_df['Processing_Status'] != 'Success'])}\")\n\n# Also keep backward compatibility - save to data/processed\nOLD_OUTPUT_DIR = \"../data/processed/\"\nos.makedirs(OLD_OUTPUT_DIR, exist_ok=True)\nprint(f\"\\n✓ Backward compatibility: Files also saved to {OLD_OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Display processing summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"PROCESSING SUMMARY\")\nprint(\"=\"*60)\n\nsummary_df = pd.DataFrame(processing_summary)\nprint(summary_df[['Survey_Name', 'Records_Processed', 'Columns_Transformed', 'Processing_Status']].to_string(index=False))\n\nprint(f\"\\nTotal records processed: {summary_df['Records_Processed'].sum()}\")\nprint(f\"Average records per survey: {summary_df['Records_Processed'].mean():.1f}\")\nprint(f\"Total columns transformed: {summary_df['Columns_Transformed'].sum()}\")\nprint(f\"Success rate: {len(summary_df[summary_df['Processing_Status'] == 'Success']) / len(summary_df) * 100:.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "if all_survey_data:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA QUALITY CHECKS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    combined_data = pd.concat(all_survey_data, ignore_index=True, sort=False)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_summary = combined_data.isnull().sum()\n",
    "    print(f\"Columns with missing values: {len(missing_summary[missing_summary > 0])}\")\n",
    "    \n",
    "    # Check for duplicate ResponseIDs within surveys\n",
    "    duplicate_check = combined_data.groupby('Survey_Name')['ResponseID'].apply(lambda x: x.duplicated().sum() if 'ResponseID' in combined_data.columns else 0)\n",
    "    print(f\"Surveys with duplicate ResponseIDs: {len(duplicate_check[duplicate_check > 0])}\")\n",
    "    \n",
    "    # Data type summary\n",
    "    print(f\"Total unique columns across all surveys: {len(combined_data.columns)}\")\n",
    "    print(f\"Numeric columns: {len(combined_data.select_dtypes(include=[np.number]).columns)}\")\n",
    "    print(f\"Text columns: {len(combined_data.select_dtypes(include=['object']).columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\n### 1. Configure Survey SQL\nFor each survey, create a folder in `sql_queries/survey_name/` containing:\n- `get_data.sql` - SQL query to extract survey data\n- `dev_notes.md` - Documentation about the SQL logic\n\nExample structure:\n```\nsql_queries/\n├── survey_1/\n│   ├── get_data.sql\n│   └── dev_notes.md\n├── survey_2/\n│   ├── get_data.sql\n│   └── dev_notes.md\n└── survey_6/      # Your new survey\n    ├── get_data.sql\n    └── dev_notes.md\n```\n\n### 2. Configure Survey Mappings\nEdit `config/survey_column_mappings.json` to specify which columns need which dictionary mappings:\n```json\n\"survey_6\": {\n  \"columns\": {\n    \"Column_Name\": \"dictionary_name\",\n    \"Another_Column\": \"another_dictionary\"\n  }\n}\n```\n\n### 3. Document SQL Logic\nIn each survey's `dev_notes.md`, document:\n- Data source (database, tables, views)\n- Column mappings and descriptions\n- SQL logic (joins, filters, transformations)\n- Data quality notes\n- Change history\n\n### 4. Available Dictionaries\nThe system includes 15 pre-defined mapping dictionaries in `config/mapping_dictionaries.json`:\n- `satisfaction_5_scale`: Very Dissatisfied → 1, ..., Very Satisfied → 5\n- `agreement_5_scale`: Strongly Disagree → 1, ..., Strongly Agree → 5\n- `frequency_5_scale`: Never → 1, ..., Always → 5\n- `quality_5_scale`: Very Poor → 1, ..., Excellent → 5\n- `yes_no_binary`: Yes → 1, No → 0\n- `true_false_binary`: True → 1, False → 0\n- `importance_5_scale`: Not Important → 1, ..., Extremely Important → 5\n- `likelihood_5_scale`: Very Unlikely → 1, ..., Very Likely → 5\n- `difficulty_5_scale`: Very Difficult → 1, ..., Very Easy → 5\n- `performance_5_scale`: Far Below Expectations → 1, ..., Far Above Expectations → 5\n- `recommendation_10_scale`: 0 → 0, ..., 10 → 10\n- `priority_3_scale`: Low → 1, Medium → 2, High → 3\n- `size_5_scale`: Much Too Small → 1, ..., Much Too Large → 5\n- `change_5_scale`: Much Worse → 1, ..., Much Better → 5\n- `awareness_4_scale`: Not Aware → 1, ..., Extremely Aware → 4\n\n### 5. Database Connection\nUpdate the `execute_query_and_load_data` function with your actual database connection details when ready to process real data.",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}