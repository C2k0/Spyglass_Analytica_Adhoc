{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Survey Analysis\n",
    "\n",
    "This notebook demonstrates techniques for analyzing survey data, including response mapping, statistical analysis, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Import"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import standard data science libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Statistical analysis\nfrom scipy import stats\n\n# Visualization settings\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\n%matplotlib inline\n\n# Import custom survey utility functions\nfrom survey_utils import *\n\n# Import column mapping dictionaries\nfrom column_mappings import *\n\n# Import Snowflake configuration and utilities\nfrom snowflake_config import snowflake_cfg, connect_to_snowflake, execute_sql_file\n\n# Path to SQL file\nSQL_FILE_PATH = os.path.join(os.path.dirname(__file__), 'snowflake_queries.sql')",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load your survey data from CSV, Excel, or other formats."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Snowflake connection configuration\n# Update this with your actual Snowflake credentials\nmy_snowflake_cfg = snowflake_cfg.copy()\nmy_snowflake_cfg.update({\n    \"account\": \"your_account_identifier\",\n    \"user\": \"your_username\",\n    \"role\": \"ANALYST\",\n    \"warehouse\": \"ANALYTICS_WH\",\n    \"database\": \"SURVEY_DATA\",\n    \"schema\": \"PUBLIC\",\n    \"private_key_path\": \"\",  # Set this to the path of your .p8 key file when ready\n})\n\n# SQL parameters for the temp table creation queries\nsql_params = {\n    \"survey_id\": \"SURVEY_2023_Q4\",  # Change as needed\n    \"start_date\": \"2023-10-01\",     # Change as needed\n    \"end_date\": \"2023-12-31\"        # Change as needed\n}\n\n# Set up Snowflake connection and temp tables\nsnowflake_conn = setup_snowflake_data(my_snowflake_cfg, SQL_FILE_PATH, sql_params)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Example: Query the temp tables\n# Replace this with your own query to load the survey data\nif snowflake_conn:\n    # Insert your custom query here to load your data\n    query = \"\"\"\n    -- Replace this with your query\n    SELECT \n        *\n    FROM \n        SURVEY_RESPONSES\n    LIMIT 10\n    \"\"\"\n    \n    # Execute the query and load results into a DataFrame\n    df = execute_query(snowflake_conn, query)\n    \n    if df is not None:\n        print(f\"Successfully loaded {len(df)} records from Snowflake\")\n        df.head()\n    else:\n        print(\"Query returned no results\")\nelse:\n    print(\"No Snowflake connection available\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Exploration\n\nExplore the dataset structure and contents. This section automatically:\n1. Generates a comprehensive summary of all columns\n2. Exports the summary to CSV and Excel files\n3. Displays key dataset characteristics",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Basic data exploration\nif 'df' in locals() and df is not None:\n    print(f\"Number of survey responses: {len(df)}\")\n    \n    # Generate comprehensive data summary\n    print(\"\\nGenerating data summary...\")\n    data_summary = generate_data_summary(df)\n    \n    # Display summary information\n    print(\"\\nData Summary:\")\n    display(data_summary)\n    \n    # Export the data summary to a file\n    export_path = os.path.join(os.path.dirname(SQL_FILE_PATH), 'data_summary.csv')\n    export_data_summary(data_summary, export_path, 'csv')\n    \n    # Also export to Excel format for better readability\n    excel_path = os.path.join(os.path.dirname(SQL_FILE_PATH), 'data_summary.xlsx')\n    export_data_summary(data_summary, excel_path, 'excel')\n    \n    # Basic DataFrame info\n    print(\"\\nDataFrame Info:\")\n    df.info()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Function to style the data summary table for better visualization\ndef style_data_summary(summary_df):\n    \"\"\"Apply styling to the data summary table.\"\"\"\n    # Define styling function\n    def highlight_fill_rate(val):\n        \"\"\"Highlight the fill rate column based on values.\"\"\"\n        if pd.isna(val):\n            return ''\n        \n        # Color-coding based on fill rate percentage\n        if val < 50:\n            return 'background-color: #ffcccc'  # Light red\n        elif val < 80:\n            return 'background-color: #ffffcc'  # Light yellow\n        else:\n            return 'background-color: #ccffcc'  # Light green\n    \n    # Apply styling\n    styled = summary_df.style.applymap(\n        highlight_fill_rate, \n        subset=['Fill Rate (%)']\n    ).set_properties(**{\n        'text-align': 'left',\n        'white-space': 'pre-wrap'\n    }).set_table_styles([\n        {'selector': 'th', 'props': [('background-color', '#eaeaea'), \n                                    ('font-weight', 'bold'),\n                                    ('text-align', 'left')]},\n        {'selector': '.row_heading', 'props': [('display', 'none')]},  # Hide index\n    ])\n    \n    return styled\n\n# Display styled data summary if available\nif 'data_summary' in locals() and not data_summary.empty:\n    styled_summary = style_data_summary(data_summary)\n    display(styled_summary)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize data completeness\nif 'data_summary' in locals() and not data_summary.empty:\n    # Sort by fill rate for better visualization\n    completeness_df = data_summary.sort_values('Fill Rate (%)', ascending=False)\n    \n    plt.figure(figsize=(12, max(6, len(data_summary) * 0.3)))\n    \n    # Create horizontal bar chart\n    bars = plt.barh(completeness_df['Column'], completeness_df['Fill Rate (%)'], \n                    color=plt.cm.viridis(completeness_df['Fill Rate (%)'] / 100))\n    \n    # Add percentage labels to the bars\n    for bar in bars:\n        width = bar.get_width()\n        label_x_pos = width + 1\n        plt.text(label_x_pos, bar.get_y() + bar.get_height()/2, f'{width:.1f}%',\n                 va='center', fontweight='bold')\n    \n    plt.xlabel('Fill Rate (%)')\n    plt.title('Data Completeness by Column')\n    plt.xlim(0, 105)  # Ensure room for percentage labels\n    plt.grid(axis='x', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    # Visualize data types distribution\n    type_counts = data_summary['Data Type'].value_counts()\n    \n    plt.figure(figsize=(8, 5))\n    plt.pie(type_counts, labels=type_counts.index, autopct='%1.1f%%', \n            shadow=True, startangle=90, colors=plt.cm.tab10.colors)\n    plt.title('Distribution of Data Types')\n    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# View response distributions for key columns\nif 'df' in locals() and df is not None:\n    # Identify columns with categorical data\n    categorical_cols = []\n    \n    for col in df.columns:\n        # Only include columns with reasonable cardinality\n        if df[col].dtype == 'object' and 1 < df[col].nunique() < 15:\n            categorical_cols.append(col)\n    \n    if categorical_cols:\n        print(\"Response distributions for key categorical columns:\")\n        for col in categorical_cols[:5]:  # Limit to first 5 to avoid too much output\n            print(f\"\\n{col.replace('_', ' ').title()} Distribution:\")\n            value_counts = df[col].value_counts().sort_index()\n            print(value_counts)\n            \n            # Optional: Visualize the distribution\n            plt.figure(figsize=(10, 6))\n            sns.countplot(y=col, data=df, order=value_counts.index)\n            plt.title(f'Distribution of {col.replace(\"_\", \" \").title()}')\n            plt.tight_layout()\n            plt.show()\n    else:\n        print(\"No suitable categorical columns found for distribution analysis.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Apply mappings to your data\n# Example: if 'response_text' exists, map it based on the question type\nif 'response_text' in df.columns and 'question_category' in df.columns:\n    # Create a new column for the numerical scores\n    df['response_score'] = None\n    \n    # Apply different mappings based on question category\n    mask = df['question_category'] == 'SATISFACTION'\n    df.loc[mask, 'response_score'] = df.loc[mask].apply(\n        lambda row: satisfaction_mapping.get(row['response_text'], None), axis=1)\n    \n    mask = df['question_category'] == 'AGREEMENT'\n    df.loc[mask, 'response_score'] = df.loc[mask].apply(\n        lambda row: likert_scale_mapping.get(row['response_text'], None), axis=1)\n    \n    mask = df['question_category'] == 'FREQUENCY'\n    df.loc[mask, 'response_score'] = df.loc[mask].apply(\n        lambda row: frequency_mapping.get(row['response_text'], None), axis=1)\n    \n    mask = df['question_category'] == 'YES_NO'\n    df.loc[mask, 'response_score'] = df.loc[mask].apply(\n        lambda row: yes_no_mapping.get(row['response_text'], None), axis=1)\n\n# Examples of mappings for specific columns\n# Uncomment and modify as needed for your specific data\n\"\"\"\n# Satisfaction columns\nfor col in satisfaction_columns:\n    if col in df.columns:\n        df[f\"{col}_score\"] = apply_mapping_if_exists(df, col, satisfaction_mapping)\n\n# Likert scale columns (agreement-based)\nlikert_cols = ['product_quality', 'ease_of_use', 'recommend', 'value_for_money']\nfor col in likert_cols:\n    if col in df.columns:\n        df[f\"{col}_score\"] = apply_mapping_if_exists(df, col, likert_scale_mapping)\n\n# Frequency columns\nfreq_cols = ['usage_frequency', 'visit_frequency', 'purchase_frequency']\nfor col in freq_cols:\n    if col in df.columns:\n        df[f\"{col}_score\"] = apply_mapping_if_exists(df, col, frequency_mapping)\n\"\"\"\n\n# Display sample of the mapped data\nif 'response_score' in df.columns:\n    df[['response_text', 'question_category', 'response_score']].head(10)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Calculate statistics for numerical columns\nif 'df' in locals() and df is not None:\n    # Identify numerical columns\n    numerical_cols = []\n    \n    # Add mapped score columns\n    score_cols = [col for col in df.columns if col.endswith('_score')]\n    numerical_cols.extend(score_cols)\n    \n    # Add any direct numerical columns\n    for col in df.columns:\n        if df[col].dtype in ['int64', 'float64'] and col not in numerical_cols:\n            numerical_cols.append(col)\n    \n    if numerical_cols:\n        print(\"Response statistics for numerical columns:\")\n        for col in numerical_cols:\n            if df[col].notna().sum() > 0:  # Only analyze if we have non-null values\n                stats = calculate_response_stats(df, col)\n                print(f\"\\n{col.replace('_score', '').replace('_', ' ').title()} Statistics:\")\n                for stat, value in stats.items():\n                    print(f\"{stat}: {value:.2f}\")\n                \n                # Optional: Create boxplot for the numerical column\n                plt.figure(figsize=(8, 6))\n                sns.boxplot(y=df[col].dropna())\n                plt.title(f'Distribution of {col.replace(\"_\", \" \").title()}')\n                plt.tight_layout()\n                plt.show()\n    else:\n        print(\"No numerical columns found for statistical analysis.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate statistics for score columns\n",
    "score_columns = [col for col in df.columns if col.endswith('_score')]\n",
    "\n",
    "for col in score_columns:\n",
    "    stats = calculate_response_stats(df, col)\n",
    "    print(f\"\\n{col.replace('_score', '').replace('_', ' ').title()} Statistics:\")\n",
    "    for stat, value in stats.items():\n",
    "        print(f\"{stat}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Plot distribution of numerical columns\nif 'df' in locals() and df is not None:\n    # Identify numerical columns\n    score_cols = [col for col in df.columns if col.endswith('_score')]\n    \n    if score_cols:\n        # Pick the first score column for demonstration\n        score_col = score_cols[0]\n        print(f\"Distribution plot for {score_col}:\")\n        plot_response_distribution(df, score_col, f'Distribution of {score_col.replace(\"_score\", \"\").replace(\"_\", \" \").title()} Scores')\n    else:\n        print(\"No score columns found for distribution plotting.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Correlation analysis for numerical columns\nif 'df' in locals() and df is not None:\n    # Identify numerical columns for correlation analysis\n    numerical_cols = []\n    \n    # Add mapped score columns\n    score_cols = [col for col in df.columns if col.endswith('_score')]\n    numerical_cols.extend(score_cols)\n    \n    # Add any direct numerical columns\n    for col in df.columns:\n        if df[col].dtype in ['int64', 'float64'] and col not in numerical_cols:\n            numerical_cols.append(col)\n    \n    if len(numerical_cols) >= 2:\n        print(\"Correlation analysis for numerical columns:\")\n        corr_matrix = correlation_matrix(df, numerical_cols)\n        print(corr_matrix.round(2))\n        \n        # Visualize correlation matrix\n        plot_correlation_heatmap(corr_matrix, 'Correlation Between Survey Metrics')\n    else:\n        print(\"Not enough numerical columns for correlation analysis.\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Segment analysis example\n# This needs to be adapted to your specific data structure\n# Replace with your own demographic variables and response variables\n\n# Example: If data is at the respondent level with multiple demographic fields\nif 'respondent_id' in df.columns and 'response_score' in df.columns:\n    # Identify demographic columns from our predefined list that exist in the dataframe\n    available_demographic_cols = [col for col in demographic_columns if col in df.columns]\n    \n    if available_demographic_cols:\n        print(\"Analyzing responses by demographic segments:\")\n        for segment in available_demographic_cols:\n            # Only perform analysis if we have multiple values in the segment\n            if df[segment].nunique() > 1:\n                # For respondent-level data, can analyze directly\n                segment_analysis_result = segment_analysis(df, segment, 'response_score')\n                print(f\"\\n{segment.replace('_', ' ').title()} Segment Analysis:\")\n                print(segment_analysis_result)\n                \n                # Visualize if possible\n                if len(segment_analysis_result) > 0 and len(segment_analysis_result) <= 10:\n                    plt.figure(figsize=(10, 6))\n                    sns.barplot(x=segment_analysis_result.index, y=segment_analysis_result.values)\n                    plt.title(f'Average Response by {segment.replace(\"_\", \" \").title()}')\n                    plt.ylabel('Average Response Score')\n                    plt.xticks(rotation=45)\n                    plt.tight_layout()\n                    plt.show()\n    else:\n        print(\"No demographic columns from the predefined list are available in the dataset.\")\n\n# Example: If data is at the question-response level and needs aggregation\nelif 'question_id' in df.columns and 'respondent_id' in df.columns and 'response_score' in df.columns:\n    # Identify demographic columns available in the data\n    available_demographic_cols = [col for col in demographic_columns if col in df.columns]\n    \n    if available_demographic_cols and 'question_category' in df.columns:\n        print(\"Analyzing responses by demographic segments and question category:\")\n        \n        # Example: Analyze average scores by demographic segment and question category\n        for segment in available_demographic_cols:\n            if df[segment].nunique() > 1:\n                # Group by segment and question category\n                grouped = df.groupby([segment, 'question_category'])['response_score'].mean().reset_index()\n                \n                # Pivot to create a table with segment rows and question category columns\n                pivot_table = grouped.pivot(index=segment, columns='question_category', values='response_score')\n                \n                print(f\"\\n{segment.replace('_', ' ').title()} by Question Category:\")\n                print(pivot_table)\n    else:\n        print(\"Required columns for segment analysis are not available.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize segment analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=satisfaction_by_age.index, y=satisfaction_by_age.values)\n",
    "plt.title('Average Satisfaction by Age Group')\n",
    "plt.ylabel('Average Satisfaction Score')\n",
    "plt.ylim(1, 5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cross-tabulation analysis example\nif 'df' in locals() and df is not None:\n    # Identify categorical columns with reasonable cardinality\n    categorical_cols = []\n    \n    for col in df.columns:\n        if df[col].dtype == 'object' and 1 < df[col].nunique() < 10:\n            categorical_cols.append(col)\n    \n    if len(categorical_cols) >= 2:\n        # Select first two categorical columns for cross-tab example\n        col1, col2 = categorical_cols[:2]\n        \n        print(f\"Cross-tabulation of {col1} by {col2}:\")\n        cross_tab = response_cross_tabulation(df, col1, col2, normalize='index')\n        print(cross_tab.round(2))\n        \n        # Visualize the cross-tabulation\n        plt.figure(figsize=(12, 8))\n        sns.heatmap(cross_tab, annot=True, cmap='viridis', fmt='.2f')\n        plt.title(f'Cross-tabulation of {col1} by {col2} (Normalized by Row)')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"Not enough categorical columns for cross-tabulation analysis.\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Free text analysis example\nif 'df' in locals() and df is not None:\n    # Identify potential text columns (longer text fields)\n    text_cols = []\n    \n    for col in df.columns:\n        if df[col].dtype == 'object':\n            # Check if this appears to be a free text field (average length > 20 chars)\n            avg_len = df[col].astype(str).str.len().mean()\n            if avg_len > 20:\n                text_cols.append(col)\n    \n    if text_cols:\n        # Example: Word count analysis on first identified text column\n        text_col = text_cols[0]\n        print(f\"Word count analysis for {text_col}:\")\n        \n        word_counts = free_text_word_count(df, text_col, min_count=2)\n        if not word_counts.empty:\n            print(word_counts.head(10))\n            \n            # Visualize top words\n            plt.figure(figsize=(12, 6))\n            sns.barplot(x=word_counts.index[:15], y=word_counts.values[:15])\n            plt.title(f'Top Words in {text_col}')\n            plt.xlabel('Word')\n            plt.ylabel('Count')\n            plt.xticks(rotation=45)\n            plt.tight_layout()\n            plt.show()\n        else:\n            print(f\"No meaningful word counts found in {text_col}\")\n    else:\n        print(\"No free text fields identified for text analysis.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get word counts from feedback\n",
    "# This is a simplified example - in practice you would want to:\n",
    "# 1. Remove stop words (common words like 'the', 'and', etc.)\n",
    "# 2. Tokenize properly, considering n-grams\n",
    "# 3. Apply stemming or lemmatization\n",
    "# 4. Consider using NLP libraries like NLTK or spaCy\n",
    "\n",
    "word_counts = free_text_word_count(df, 'feedback', min_count=5)\n",
    "word_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights and Recommendations\n",
    "\n",
    "Summarize key findings from the survey analysis and provide recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis above, key insights include:\n",
    "\n",
    "1. [Add insights based on actual analysis]\n",
    "2. ...\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "1. [Add recommendations based on insights]\n",
    "2. ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}